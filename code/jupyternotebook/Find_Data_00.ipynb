{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../myenv/lib/python3.7/site-packages')\n",
    "sys.path.append('~/Library/Caches/Homebrew/downloads/bc3fcd9890493f143dbada17d9627acd14efd68dfb72b195a7abca9df3f93361--wkhtmltox-0.12.6-2.macos-cocoa.pkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from japanera import Japanera\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import camelot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time: # change class name to Time\n",
    "    \n",
    "    \"\"\"Time stamp for code execution\"\"\"\n",
    "        \n",
    "    def get_current_time(self):\n",
    "        now = datetime.datetime.now()\n",
    "        return(now)\n",
    "    \n",
    "    def get_start_time(self):\n",
    "        # Start time\n",
    "        start_time = self.get_current_time()\n",
    "        return(start_time)\n",
    "       \n",
    "    def get_end_time(self):\n",
    "        # End time\n",
    "        end_time = self.get_current_time()\n",
    "        elapsed_time = end_time - self.get_start_time()\n",
    "        return(end_time, elapsed_time)\n",
    "        \n",
    "    def print_start(self):\n",
    "        print('--------Start Script--------')\n",
    "        print('--------Start Time: ' + self.get_start_time().strftime('%Y-%m-%d %H:%M:%S') + '-------\\n')\n",
    "\n",
    "    def print_end(self):\n",
    "        print('Total ' + str(self.get_end_time()[1].seconds) + ' [sec]')\n",
    "        print('-----End Time : ' + self.get_end_time()[0].strftime('%Y-%m-%d %H:%M:%S') + ' ---------')\n",
    "        print('-----END SCRIPT------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contents():\n",
    "    \n",
    "    \"\"\"A class to find appropriate link contents\"\"\"\n",
    "    \n",
    "    from urllib.request import urlopen\n",
    "    from bs4 import BeautifulSoup\n",
    "        \n",
    "    def __init__(self, url):\n",
    "        \"\"\"Initialize url attribute.\n",
    "        To access the attributes of an instance, \n",
    "        use dot notation\"\"\"\n",
    "        self.url = url\n",
    "        \n",
    "    def open_url(self):\n",
    "        html = urlopen(self.url)\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        return(soup)\n",
    "    \n",
    "    def get_urls(self):\n",
    "        soup = self.open_url()\n",
    "        all_links = soup.find_all(\"a\")\n",
    "        return(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    \n",
    "    \"\"\"Manipulate data with scraped content\"\"\"\n",
    "    \n",
    "    def find_data(self, test_tables):\n",
    "        for table in test_tables:\n",
    "            table_content = table.find('tr')\n",
    "            print(table_content.find('th'))\n",
    "#             print(table_content)\n",
    "            if (table_content.find('td') and table_content.find('td').text == '国・地域'):\n",
    "                found = table\n",
    "                break\n",
    "            elif (table_content.find('th') and table_content.find('th').text == '国・地域'):\n",
    "                found = table\n",
    "                break\n",
    "            elif (table_content.find('td') and table_content.find('td').text == '　'):\n",
    "                found = table\n",
    "                break\n",
    "            elif (table_content.find('td') and '国・地位' in table_content.find('td').text):\n",
    "                found = table\n",
    "                break\n",
    "            else:\n",
    "                print(\"Skip table: Not labeled '国・地域'\") \n",
    "        try:\n",
    "            found\n",
    "        except NameError:\n",
    "            print(\"Error: Did not find table labeled '国・地域'\")\n",
    "        else:\n",
    "            print(\"Found table labeled '国・地域'\")\n",
    "            return(found)\n",
    "\n",
    "    def clean_data(self, data):\n",
    "        cleantext = [text for text in data.stripped_strings]\n",
    "        print(\"Changed data to str type.\")\n",
    "        print(\"Cleanted text.\")\n",
    "        return(cleantext)\n",
    "    \n",
    "    def clean_int(self, date_str):\n",
    "        cleaned = ''\n",
    "        for i in date_str:\n",
    "            try: \n",
    "                clean = str(int(i))\n",
    "            except ValueError:\n",
    "                clean = i\n",
    "            cleaned += clean\n",
    "        return(cleaned)\n",
    "             \n",
    "    def extract_pdf_table(self, url):\n",
    "        tables = camelot.read_pdf(url)\n",
    "        return(tables)\n",
    "    \n",
    "    def data_dict_df(self, list_data_values, list_col_names):\n",
    "        data_dict = {key: [] for key in list_col_names}\n",
    "        for col, data in enumerate(list_data_values):\n",
    "            for value in range(len(data)):\n",
    "                data_dict[list_col_names[col]].append(data[value].text.strip())\n",
    "        df = pd.DataFrame(data_dict)    \n",
    "        return(df)\n",
    "\n",
    "    def export_table(self, table, file_name, file_type):\n",
    "        # export individually\n",
    "        path = '../../data/raw/'\n",
    "#         path = '~/Desktop/missing_csv/'\n",
    "        \n",
    "        if file_type == \"csv\":\n",
    "            file_type = \".csv\"\n",
    "            table.to_csv(path + file_name + file_type)\n",
    "        elif file_type == \"txt\":\n",
    "            file_type = \".txt\"\n",
    "            with open(path + file_name + file_type, \"w\") as text_file:\n",
    "                text_file.write(table)\n",
    "        else:\n",
    "            print('Error: Indicate data, filename, filetype - txt or csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_links(url, month):\n",
    "    data = Data()\n",
    "    \n",
    "    contents_00 = Contents(url)\n",
    "    all_links = contents_00.get_urls()\n",
    "    data_links = []\n",
    "\n",
    "    ###Step 1: Find link to data###\n",
    "    print(\"Step 1: Find link to data\")\n",
    "\n",
    "    for link in all_links:\n",
    "        if ['m-listLinkMonth__link'] == link.get(\"class\"):\n",
    "            if 'https://www.mhlw.go.jp/stf/houdou/houdou_list_2020' in link.get(\"href\"):\n",
    "                data_links.append(link.get(\"href\"))\n",
    "\n",
    "    # number of url to monthly data\n",
    "    print(\"Number of links: \" + str(len(data_links)))\n",
    "\n",
    "    # choose monthly data here [-1] is the most recent \n",
    "    url = sorted(data_links)[month - 1] #############change month#################\n",
    "    print(\"Url for a specific month: \" + url)\n",
    "\n",
    "    contents_01 = Contents(url)\n",
    "    all_links = contents_01.get_urls()\n",
    "    target_links = []\n",
    "\n",
    "    for link in all_links:\n",
    "        for child in link.descendants:\n",
    "            if '新型コロナウイルス感染症の現在の状況と厚生労働省の対応について（令和２年' in child:\n",
    "                target = child.parent.previous_element.previous_element.previous_element\n",
    "                target_link = 'https://www.mhlw.go.jp' + target.get('href')\n",
    "                target_links.append(target_link)\n",
    "    #     url = target_links[4] #change day in month\n",
    "    print(\"Number of links: \" + str(len(target_links)))\n",
    "    return(target_links)\n",
    "\n",
    "def corona_country(target_links):\n",
    "\n",
    "    data = Data()\n",
    "    \"\"\"国別コロナ数\"\"\"\n",
    "    \n",
    "    for url in target_links:\n",
    "        print(\"Chosen url to data: \" + url)\n",
    "        ###Step 2: Get content of data###\n",
    "        print(\"Step 2: Get content of data\")\n",
    "\n",
    "        content3 = Contents(url)\n",
    "        soup = content3.open_url()\n",
    "\n",
    "        # Get title\n",
    "        title = str(soup.title)\n",
    "        print(\"This is the title of url: \" + title)\n",
    "\n",
    "        ###Step 3: Format Date of data###\n",
    "        print(\"Step 3: Format Date of data\")\n",
    "\n",
    "        # Fetch and format date 令和02年5月30日版\n",
    "        date = title[title.find(\"令和\"):title.find(\"日\")+1]\n",
    "        date = data.clean_int(date)\n",
    "        date_formatted = date[:2] + '0' + date[2:]\n",
    "        print(\"Check date format: \" + date_formatted)\n",
    "        janera = Japanera()\n",
    "        date_reformat = janera.strptime(date_formatted, '%-E%-O年%m月%d日')\n",
    "        year = str(date_reformat[0].year)\n",
    "        month = str(date_reformat[0].month)\n",
    "        day = str(date_reformat[0].day)\n",
    "        print(\"Formatted date: \" + date_formatted)\n",
    "\n",
    "        ###Step 4: Find table and clean table###\n",
    "        print(\"Step 4: Find table and clean table\")\n",
    "\n",
    "        # Get table\n",
    "        print(\"Numbers of tables found: \" + str(len(soup.find_all('table'))))\n",
    "        find = soup.find_all('table')\n",
    "        find_data = data.find_data(find)\n",
    "        clean_data = data.clean_data(find_data)\n",
    "        print(clean_data)\n",
    "        clean_data = str(clean_data)\n",
    "\n",
    "        ###Step 5: Choose file name and export data###\n",
    "        print(\"Step 5: Choose file name and export data\")\n",
    "\n",
    "        date_formatted = '{}_{}_{}'.format(year,month,day)\n",
    "        file_name_00 = date_formatted + '_corona_country'\n",
    "        data.export_table(clean_data, file_name_00, \"txt\")\n",
    "        print(\"Data exported as: \" + file_name_00)\n",
    "\n",
    "def pcr_japan(target_links):\n",
    "    data = Data()\n",
    "\n",
    "    \"\"\"国内における都道府県別のPCR検査陽性者数\"\"\"\n",
    "\n",
    "    ###Step 2: Get content of data###\n",
    "    print(\"Step 2: Get content of data\")\n",
    "\n",
    "    for url in target_links:\n",
    "        contents_02 = Contents(url)\n",
    "        soup = contents_02.open_url()\n",
    "\n",
    "        links = []\n",
    "        for link in soup.find_all(\"a\"):\n",
    "            for child in link.descendants:\n",
    "                if '国内における都道府県別のPCR検査陽性者数（2020年' in child:\n",
    "#                 if '新型コロナウイルス陽性者数とPCR検査実施人数（都道府県別）（2020年' in child: #april, march\n",
    "#                 if '国内における都道府県別のPCR検査陽性者数（2020年' in child: #missing data\n",
    "                    target_link = 'https://www.mhlw.go.jp' + link.get(\"href\")\n",
    "                    links.append(child)\n",
    "                    links.append(target_link)\n",
    "\n",
    "        info = sorted(list(set(links)))\n",
    "        print(sorted(info))\n",
    "         \n",
    "        print(\"length of list: \" + str(len(info)))\n",
    "        \n",
    "        if len(info) > 2:\n",
    "            info.pop(0)\n",
    "        elif len(info) != 0:\n",
    "            info \n",
    "        else:\n",
    "            print(\"Error: Cannot find link to data.\")\n",
    "            continue\n",
    "            \n",
    "        date = info[1][info[1].find(\"年\")-4:info[1].find(\"日\")+1]\n",
    "        date = data.clean_int(date)\n",
    "        print(\"Cleaned date: \" + date)\n",
    "        date_format = datetime.datetime.strptime(date, '%Y年%m月%d日')\n",
    "        date_formatted = '{}_{}_{}'.format(date_format.year,date_format.month,date_format.day)\n",
    "        print(\"date formatted: \" + date_formatted)\n",
    "\n",
    "        file = info[0]\n",
    "        print(\"PDF file extracted: \" + file)\n",
    "        \n",
    "        tables = data.extract_pdf_table(file)\n",
    "        # number of tables extracted\n",
    "        print(\"Total tables extracted:\", tables.n)\n",
    "        \n",
    "        if tables.n == 0:\n",
    "            print(\"Error : No tables Found\")\n",
    "        else:\n",
    "            tables\n",
    "        for i, table in enumerate(tables):\n",
    "            extracted = table.df\n",
    "            file_name = date_formatted + '_corona_jp_{}'.format(i+1)\n",
    "            data.export_table(extracted, file_name, \"csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Start Script--------\n",
      "--------Start Time: 2021-01-02 10:10:48-------\n",
      "\n",
      "Step 1: Find link to data\n",
      "Number of links: 12\n",
      "Url for a specific month: https://www.mhlw.go.jp/stf/houdou/houdou_list_202012.html\n",
      "Number of links: 31\n",
      "Step 2: Get content of data\n",
      "['https://www.mhlw.go.jp/content/10906000/000713248.pdf', '国内における都道府県別のPCR検査陽性者数（2020年12月31日掲載分）']\n",
      "length of list: 2\n",
      "Cleaned date: 2020年12月31日\n",
      "date formatted: 2020_12_31\n",
      "PDF file extracted: https://www.mhlw.go.jp/content/10906000/000713248.pdf\n",
      "Total tables extracted: 2\n",
      "Total 86399 [sec]\n",
      "-----End Time : 2021-01-02 10:10:52 ---------\n",
      "-----END SCRIPT------\n"
     ]
    }
   ],
   "source": [
    "class Main():\n",
    "    \n",
    "    \"\"\"Get data for corona deaths by country\"\"\"\n",
    "    time = Time()\n",
    "    time.print_start()  \n",
    "    \n",
    "    ###########################################\n",
    "    #monthly 報道発表資料 (main link)\n",
    "    url = \"https://www.mhlw.go.jp/stf/houdou/index.html\"\n",
    "    #set month of data\n",
    "    month = 12 #For some months, data doesn't exist OR posted in different structure.\n",
    "    ###########################################\n",
    "#     url = target_links[0] #most recent date\n",
    "\n",
    "    target_links = get_target_links(url, month)\n",
    "    target_links = [target_links[0]]\n",
    "    \n",
    "    #国内における都道府県別のPCR検査陽性者数#\n",
    "    pcr_japan(target_links)\n",
    "\n",
    "    #国別コロナ数#\n",
    "#     corona_country(target_links) #Only half of Feb works. \n",
    "\n",
    "    time.print_end()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<meta content=\"都道府県の人口ランキング・面積ランキング・人口密度ランキングです。人口は、2020年10月1日の推計人口によります。推計人口とは、直近の国勢調査確定人口を基に、その後の人口動向を他の人口関連資料から得て算出するもので、住民基本台帳人口とは違い、より実際の人口に近い数が算出されます。\" name=\"description\"/>\n",
      "2020年10月1日\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "#国内における都道府県別の人口#\n",
    "##########################\n",
    "\n",
    "data = Data()\n",
    "\n",
    "def get_file_n(date_formatted, col_names):\n",
    "    file_names = ['{}_{}'.format(date_formatted, name) for name in col_names]\n",
    "    return(file_names)\n",
    "\n",
    "# def population_prefecture_japan():\n",
    "# most recent population data\n",
    "# url = 'https://uub.jp/rnk/p_j.html'\n",
    "# Relavent dates to use for corona data \n",
    "# url = 'https://uub.jp/rnk/rnk.cgi?T=p&S=j&B=20191001'\n",
    "url = 'https://uub.jp/rnk/rnk.cgi?T=p&S=j&B=20201001'\n",
    "\n",
    "\n",
    "content5 = Contents(url)\n",
    "soup = content5.open_url()\n",
    "description = soup.find('meta', attrs={'name': 'description'})\n",
    "print(description)\n",
    "desc_cleaned = description.prettify()\n",
    "date = desc_cleaned[desc_cleaned.find(\"年\")-4:desc_cleaned.find(\"日\")+1]\n",
    "print(date)\n",
    "date_format = datetime.datetime.strptime(date, '%Y年%m月%d日')\n",
    "date_formatted = '{}_{}_{}'.format(date_format.year,date_format.month,date_format.day)\n",
    "\n",
    "\n",
    "file_name1, file_name2, file_name3 = get_file_n(date_formatted, [\"population\", \"area\", \"popDensity\"])\n",
    "\n",
    "# Find tables\n",
    "find = soup.find_all('td') \n",
    "table1 = find[3:147] # 都道府県別　人口データ\n",
    "table2 = find[147:147 + 144] # 都道府県別　面積データ\n",
    "table3 = find[147 + 144:147 + 144 + 144] # 都道府県別　人口密度データ\n",
    "\n",
    "# Clean data\n",
    "# Get prefectures\n",
    "test1 = table1[4::3]\n",
    "test2 = table2[4::3]\n",
    "test3 = table3[4::3]\n",
    "\n",
    "# Get data\n",
    "pop = table1[5::3]\n",
    "area = table2[5::3]\n",
    "den = table3[5::3]\n",
    "\n",
    "# Construct dataframe\n",
    "export_df1 = data.data_dict_df([test1, pop], [\"pref\", \"population\"])\n",
    "export_df2 = data.data_dict_df([test2, area], [\"pref\", \"area\"])\n",
    "export_df3 = data.data_dict_df([test3, den], [\"pref\", \"popDensity\"])\n",
    "\n",
    "# Export dataframe\n",
    "data.export_table(export_df1, file_name1, \"csv\")\n",
    "data.export_table(export_df2, file_name2, \"csv\")\n",
    "data.export_table(export_df3, file_name3, \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     ##############\n",
    "#     #国内の自殺者数#\n",
    "      ##############  \n",
    "#     # file2 = 'https://www.npa.go.jp/safetylife/seianki/jisatsu/R02/R01_jisatuno_joukyou.pdf'\n",
    "#     # tables = camelot.read_pdf(file2, pages='33')\n",
    "#     # # number of tables extracted\n",
    "#     # print(\"Total tables extracted:\", tables.n)\n",
    "#     # tables[0].df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
